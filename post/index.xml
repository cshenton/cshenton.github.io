<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Lean Data Guy</title>
    <link>http://leandataguy.com/post/index.xml</link>
    <description>Recent content in Posts on Lean Data Guy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>My Name</copyright>
    <lastBuildDate>Sat, 04 Mar 2017 16:30:51 +1100</lastBuildDate>
    <atom:link href="http://leandataguy.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The Data Warehouse is an Antipattern</title>
      <link>http://leandataguy.com/2017/03/04/whats-the-warehouse-for/</link>
      <pubDate>Sat, 04 Mar 2017 16:30:51 +1100</pubDate>
      
      <guid>http://leandataguy.com/2017/03/04/whats-the-warehouse-for/</guid>
      <description>

&lt;h1 id=&#34;data-plumbing&#34;&gt;Data Plumbing&lt;/h1&gt;

&lt;p&gt;From startups to enterprise, the term &amp;ldquo;Data Warehouse&amp;rdquo; has a sort of hallowed quality to it. It is, after all, where all that &lt;em&gt;valuable&lt;/em&gt; data foolishly discarded by operational systems ends up. It&amp;rsquo;s the only place to get a &lt;em&gt;360 degree customer view&lt;/em&gt; in an increasingly service-oriented world, and can even be used by those mysterious data scientists, or (if we&amp;rsquo;re so lucky), for doing &lt;strong&gt;BIG&lt;/strong&gt; Data.&lt;/p&gt;

&lt;p&gt;And whether you call it a data warehouse, a data lake, or a series of data marts; whether you&amp;rsquo;re getting the data there by ETL or ELT, the pattern&amp;rsquo;s much the same anywhere you go:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Grab the binary logs / table extracts from your production databases&lt;/li&gt;
&lt;li&gt;Grab all the weblogs and 3rd party SaaS data&lt;/li&gt;
&lt;li&gt;(optional) transform it into a mostly logical data model&lt;/li&gt;
&lt;li&gt;Put it all in one place so it can be queried together&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although the first steps might be a bit trickier, say, if you&amp;rsquo;re using SAP business objects (my condolences) or there are some cowboys running Access databases in prod (my commiserations), the end result is much the same:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An expensive, specialised database which is miserably coupled to every data producing service in your company.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-monolith-returns&#34;&gt;The Monolith Returns&lt;/h1&gt;

&lt;p&gt;Which is exactly what we were trying to avoid when we organised operations as a series of interconnected services. The reason this is bad is that the data warehouse exists on the assumption that lots of people need it. Departmental reporting needs it, BI needs it, Analytics needs it, those Data Scientists need it. At the same time, since it is tightly coupled not just to one, but to multiple services, a single schema migration in an upstream service can easily frustrate or even break the efforts of those teams dependent on it.&lt;/p&gt;

&lt;h3 id=&#34;let-services-be-services&#34;&gt;Let Services be Services&lt;/h3&gt;

&lt;p&gt;However, the services whose production databases the warehouse replicates manage to avoid breaking after every database migration. This is because they operate on a fixed &lt;strong&gt;service contract&lt;/strong&gt;, usually in the form of a documented API that passes messages in JSON, or (if you like your APIs enterprisey) XML.&lt;/p&gt;

&lt;p&gt;This means that each service knows which resources and message schemas its consumers depend on, which enables it to make changes and add functionality with impunity, so long as that contract is maintained. This means that underlying database schemas, and even database and application runtimes can be changed without breaking anything.&lt;/p&gt;

&lt;p&gt;Without breaking anything, that is, except the data warehouse. By directly coupling to the database rather than the API, the warehouse breaks that service contract and makes itself more fragile as a service.&lt;/p&gt;

&lt;h1 id=&#34;what-s-to-be-done&#34;&gt;What&amp;rsquo;s to be Done?&lt;/h1&gt;

&lt;p&gt;This is a bad situation. In my current job, I&amp;rsquo;ve worked with analytics teams whose predictive models were frequently being broken by the very department that relied on them, and the root cause was that their source data was too tightly coupled to that department&amp;rsquo;s application database. However, I&amp;rsquo;ve also seen teams running analytical queries against operational databases, which is at best slow and at worse will break things.&lt;/p&gt;

&lt;p&gt;And the goals of warehousing data are good ones:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ensuring no potentially valuable information is discarded by an operational system&lt;/li&gt;
&lt;li&gt;Unifying data across services to answer questions that can&amp;rsquo;t be answered through any single service API or database&lt;/li&gt;
&lt;li&gt;Being a go to point for data used for modelling purposes&lt;/li&gt;
&lt;li&gt;Providing database technologies optimised for analytical queries&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So why are data warehouses a hornet&amp;rsquo;s nest of architectural antipatterns? Precisely because they try to solve all these important problems at once.&lt;/p&gt;

&lt;p&gt;Over the next few months, I&amp;rsquo;ll be breaking these goals down and looking at solutions that are a little less monolithic than a centralised data warehouse. When I do, I&amp;rsquo;ll be coming back and updating this article, which for now will have to settle for complaining about a problem without proposing a solution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stop Using R</title>
      <link>http://leandataguy.com/2017/02/03/stop-using-r/</link>
      <pubDate>Fri, 03 Feb 2017 20:25:49 +1100</pubDate>
      
      <guid>http://leandataguy.com/2017/02/03/stop-using-r/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;This article is for data scientists (those folks who build predictive systems and squeeze value out of data in business settings). If you&amp;rsquo;re an academic, an applied statistician, or if you otherwise work with a static dataset for extended periods of time, this probably doesn&amp;rsquo;t apply to you, but you might enjoy it anyway.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As data scientists, we do pretty cool things. We predict, we personalise, we automate. However, at the end of the day we&amp;rsquo;re here because we deliver value, and if anything is more valuable than a great model, it&amp;rsquo;s a great model that runs itself.&lt;/p&gt;

&lt;p&gt;Letting your models free, by running them in production and making their results available, creates value that scales with the number of people using them. Running it at your interpreter means its value is a finite as your time.&lt;/p&gt;

&lt;p&gt;More importantly, production models are stable platforms that applications can be built on top of. And because the model learns so too does the application. That&amp;rsquo;s all exciting, but it&amp;rsquo;s not easy, and R, though it is a popular language among data scientists, is not up to the task.&lt;/p&gt;

&lt;h1 id=&#34;r-can-t-walk-and-chew-gum&#34;&gt;R Can&amp;rsquo;t Walk and Chew Gum&lt;/h1&gt;

&lt;p&gt;As a language, R is pretty mature, it has client libraries for most big data tools under the sun, it has its own dashboarding framework, and it has  packages like dplyr, which make once off data munging a real pleasure. But that&amp;rsquo;s just the problem, R is very much a &amp;ldquo;once off&amp;rdquo; kind of language. It&amp;rsquo;s great for writing sets of instructions to the computer (load this, alter this, train this, graph this), but it doesn&amp;rsquo;t encourage the kind of abstraction that leads to &lt;code&gt;DRY&lt;/code&gt; (Don&amp;rsquo;t Repeat Yourself) code.&lt;/p&gt;

&lt;p&gt;So exactly what is R missing?&lt;/p&gt;

&lt;h3 id=&#34;a-command-line-package-manager&#34;&gt;A command line package manager&lt;/h3&gt;

&lt;p&gt;This is the smallest gripe here, but it&amp;rsquo;s an important one. Bad dependency management is the kind of small thing which can cause big problems that are a headache to troubleshoot. Within R packages, this is handled by a &lt;code&gt;Depends&lt;/code&gt; field, but for an R application intended for running and not importing, packaging feels like an awkward solution. Projects like &lt;a href=&#34;https://github.com/rstudio/packrat&#34;&gt;packrat&lt;/a&gt; take a step towards fixing this, by mimicking &lt;code&gt;virtualenv&lt;/code&gt; in python, but the real issue is that, in R, packages have to be installed from inside the interpreter.&lt;/p&gt;

&lt;p&gt;This is bad. For one, the default &lt;code&gt;install.packages&lt;/code&gt; doesn&amp;rsquo;t support installing a particular version of a package. If you need to do that (i.e. if a package update introduces non backwards-compatible changes that break your code) &lt;a href=&#34;http://stackoverflow.com/questions/17082341/installing-older-version-of-r-package&#34;&gt;you need to import another package&lt;/a&gt;, specifically &lt;code&gt;devtools&lt;/code&gt;, and call a completely different install function &lt;code&gt;install.versions&lt;/code&gt;. Crazy! The functionality is technically there, but it&amp;rsquo;s not surfaced in the core language, which is a bit of a theme with R.&lt;/p&gt;

&lt;p&gt;Like I said, this is a small gripe, and more a stylistic one, but it makes deploying R code a pain, and makes for &lt;a href=&#34;https://github.com/glamp/r-docker/blob/master/Dockerfile&#34;&gt;really ugly Dockerfiles&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;a-way-to-organise-your-code&#34;&gt;A way to organise your code&lt;/h3&gt;

&lt;p&gt;Collaboration is an important part of writing production code, and having a navigable codebase makes for easy collaboration. In a good python codebase, dependencies are clearly namespaced through modules and subpackages, and (python 2 to 3 nuances aside) naming conventions in import statements map nicely to directory structures in the codebase as a whole.&lt;/p&gt;

&lt;p&gt;For example, if I have a python callable which includes&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    from mypackage.models.input_data import Parameter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know to look for the &lt;code&gt;Parameter&lt;/code&gt; class in &lt;code&gt;input_data.py&lt;/code&gt; within the &lt;code&gt;models&lt;/code&gt; subfolder. Easy.&lt;/p&gt;

&lt;p&gt;This is not the case in R, and the most &lt;a href=&#34;https://github.com/topepo/caret/tree/master/models/files&#34;&gt;popular&lt;/a&gt; R &lt;a href=&#34;https://github.com/hadley/dplyr/tree/master/R&#34;&gt;packages&lt;/a&gt;  are an &lt;a href=&#34;https://github.com/tidyverse/ggplot2/tree/master/R&#34;&gt;upsetting&lt;/a&gt; testament to this.&lt;/p&gt;

&lt;h3 id=&#34;a-flexible-testing-framework&#34;&gt;A flexible testing framework&lt;/h3&gt;

&lt;p&gt;There is not much of a culture of writing tests among data scientists, and there are good reasons for this. The first is that many data scientists write scripts which comprise a series of function calls that just pass state to the next function call, like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    data1 &amp;lt;- some_import_function(some_source)
    data2 &amp;lt;- some_import_function(other_source)

    combined_data &amp;lt;- join_function(data1, data2)
    cleaned_data &amp;lt;- cleaner_function(combined_data)
    formatted_data &amp;lt;- formatting_function(cleaned_data)

    export(formatted_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(This is part of the reason why dplyr&amp;rsquo;s pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; operator is so useful in R, but also makes most data scientists&amp;rsquo; code totally unreadable. As a further aside, this antipattern is a result of many data scientists coming from an academic background where STATA and SAS&amp;rsquo;s limited scripting languages encourage this)&lt;/p&gt;

&lt;p&gt;When this is what you&amp;rsquo;re writing all day, building up the habit of testing code isn&amp;rsquo;t easy.&lt;/p&gt;

&lt;p&gt;The second is that when data scientists are writing production systems, they usually depend on a fair bit of external tooling. My first experience with this was writing an API to expose some geographically linked data we were using in a few different places. The application relied on the database to efficiently match geographic boundaries, so I was thrown right in the deep end, writing integration tests for a specific database extension. In the end I got it done, because writing database backed applications in python is par for the course, so python&amp;rsquo;s test framwork &lt;code&gt;pytest&lt;/code&gt; made writing those tests easy.&lt;/p&gt;

&lt;p&gt;This is not the case in R, in which the &lt;code&gt;RUnit&lt;/code&gt; test framework is sort of it. Its lack of flexibility is one thing but, like most R packages, it is allergic to decent documentation, providing only a &lt;a href=&#34;https://cran.r-project.org/web/packages/RUnit/RUnit.pdf&#34;&gt;pdf&lt;/a&gt; of function references. Compare this to &lt;a href=&#34;http://doc.pytest.org/en/latest/&#34;&gt;pytest&lt;/a&gt;, which has copious examples with proper syntax highlighting, and a navigation system beyond a table of contents, yeesh.&lt;/p&gt;

&lt;h3 id=&#34;a-framework-for-building-rest-apis&#34;&gt;A framework for building REST APIs&lt;/h3&gt;

&lt;p&gt;Python is not just a language for statistical computing, it&amp;rsquo;s a great general scientific computing language, and though not as in vogue as ruby or modern javascript frameworks like node and react, is a solid language for web development as well.&lt;/p&gt;

&lt;p&gt;This means it has fully fledged web frameworks, like &lt;code&gt;django&lt;/code&gt;, and &lt;code&gt;flask&lt;/code&gt;, for building web applications, and lighter weight ones, like &lt;code&gt;falcon&lt;/code&gt;, and &lt;code&gt;hug&lt;/code&gt;, for building standalone HTTP APIs.&lt;/p&gt;

&lt;p&gt;So why should you care? You&amp;rsquo;re a data scientist, not a web developer. Well if an application is going to make use of your model, it either needs to be directly integrated into that application&amp;rsquo;s codebase (a story for another time), or it needs to provide a consistent network interface for accessing its results. HTTP APIs are the de facto standard for doing just that, and are used by python developers for a heck of a lot more than statistical models. This means the web frameworks in python are mature, and also provide suppport for python&amp;rsquo;s testing frameworks.&lt;/p&gt;

&lt;p&gt;R has nothing approaching this. The closest that exists is &lt;code&gt;plumber&lt;/code&gt;, which uses decorators to map R functions to HTTP verbs, and which depends on &lt;code&gt;httpuv&lt;/code&gt; which handles basic HTTP requests, but these are not production ready (read, don&amp;rsquo;t use a proper web server).&lt;/p&gt;

&lt;h1 id=&#34;so-what-now&#34;&gt;So What Now?&lt;/h1&gt;

&lt;p&gt;Despite the provocative title, this post is not about discouraging you from using R. It&amp;rsquo;s a great language for exploratory work, and since we often deal with complex data sources, that kind of exploration is best done on the fly. And while R makes it very hard to write quality code, python makes it easy to write bad code. If your entire body of work in python looks like my joke R script above, and is devoid of a single test, that&amp;rsquo;s probably a sign you&amp;rsquo;re not using the python ecosystem to its fullest.&lt;/p&gt;

&lt;p&gt;But unlike R, the python ecosystem is there, even if you&amp;rsquo;re not using it. So when the time comes to build a production model that will be useful and lasting, R will simply not cut it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>